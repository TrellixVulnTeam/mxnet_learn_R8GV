{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用nn.Sequential 定义一个多层感知机\n",
    "\n",
    "import sys\n",
    "from mxnet import init, gluon, nd\n",
    "from mxnet.gluon import nn\n",
    "\n",
    "class MLP(nn.Block):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MLP, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.hidden = nn.Dense(4)\n",
    "            self.output = nn.Dense(2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.output(nd.relu(self.hidden(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = nd.random.uniform(shape=(3, 5))\n",
    "try:\n",
    "    net = MLP()\n",
    "    net.initialize()\n",
    "    net(x)\n",
    "except RuntimeError as err:\n",
    "    sys.stderr.write(str(err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 访问模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: \n",
      "[[-0.007007   -0.0196689   0.01582889]\n",
      " [-0.00881553  0.0563288   0.02766836]]\n",
      "<NDArray 2x3 @cpu(0)> \n",
      " grad: \n",
      "[[ 0.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "<NDArray 2x3 @cpu(0)> \n",
      " name: good_param\n"
     ]
    }
   ],
   "source": [
    "my_param = gluon.Parameter('good_param', shape=(2, 3)) # 使用 Parameter 来定义参数\n",
    "my_param.initialize()\n",
    "print('data:', my_param.data(), '\\n grad:', my_param.grad(), '\\n name:', my_param.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden layer name: mlp2_dense0 \n",
      "weight:  Parameter mlp2_dense0_weight (shape=(4, 5), dtype=<class 'numpy.float32'>) \n",
      "bias:  Parameter mlp2_dense0_bias (shape=(4,), dtype=<class 'numpy.float32'>)\n"
     ]
    }
   ],
   "source": [
    "#访问开头定义的多层感知机的各项参数：权重weight, 偏差bias\n",
    "w = net.hidden.weight\n",
    "b = net.hidden.bias\n",
    "print('hidden layer name:', net.hidden.name, '\\nweight: ', w,'\\nbias: ', b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight: \n",
      "[[ 0.00286685  0.03927409  0.02504314 -0.05344158  0.03088857]\n",
      " [ 0.01958894  0.01148278 -0.04993054  0.00523225  0.06225365]\n",
      " [ 0.03620619  0.00305876 -0.05517294 -0.01194733 -0.00369594]\n",
      " [-0.03296221 -0.04391347  0.03839272  0.03316854 -0.00613896]]\n",
      "<NDArray 4x5 @cpu(0)> \n",
      "weight grad: \n",
      "[[ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]]\n",
      "<NDArray 4x5 @cpu(0)> \n",
      "bias: \n",
      "[ 0.  0.  0.  0.]\n",
      "<NDArray 4 @cpu(0)> \n",
      "bais grad: \n",
      "[ 0.  0.  0.  0.]\n",
      "<NDArray 4 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "#参数的值和梯度\n",
    "print('weight:', w.data(), '\\nweight grad:', w.grad(), '\\nbias:',b.data(), '\\nbais grad:', b.grad())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlp2_ (\n",
      "  Parameter mlp2_dense0_weight (shape=(4, 5), dtype=<class 'numpy.float32'>)\n",
      "  Parameter mlp2_dense0_bias (shape=(4,), dtype=<class 'numpy.float32'>)\n",
      "  Parameter mlp2_dense1_weight (shape=(2, 4), dtype=<class 'numpy.float32'>)\n",
      "  Parameter mlp2_dense1_bias (shape=(2,), dtype=<class 'numpy.float32'>)\n",
      ")\n",
      "\n",
      "[ 0.  0.  0.  0.]\n",
      "<NDArray 4 @cpu(0)>\n",
      "\n",
      "[ 0.  0.  0.  0.]\n",
      "<NDArray 4 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "# 通过collect_params 来访问\n",
    "params = net.collect_params()\n",
    "print(params)\n",
    "print(params['mlp2_dense0_bias'].data())\n",
    "print(params.get('dense0_bias').data())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 初始化模型参数\n",
    "\n",
    "在gluon中，模型的参数总是默认初始化为0。当我们对整个模型所有参数做初始化时，模型下权重参数的所有元素为[-0.07, 0.07]之间均匀分布的随机数。我们也可以使用其他初始化方法。以下例子使用了均值为0，标准差为0.02的正太分布来随机初始化模型中所有层的权重参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden weight: \n",
      "[[ 0.00204577  0.02359052 -0.01088966 -0.00684989  0.02217279]\n",
      " [ 0.02551372 -0.03666487 -0.01057974  0.01203553  0.00832525]\n",
      " [ 0.01018762 -0.048427   -0.00321659  0.00771733 -0.00632596]\n",
      " [ 0.02594279 -0.00826195  0.01191583  0.0098195   0.01036525]]\n",
      "<NDArray 4x5 @cpu(0)> \n",
      "hidden bias: \n",
      "[ 0.  0.  0.  0.]\n",
      "<NDArray 4 @cpu(0)> \n",
      "output weight: \n",
      "[[ 0.00784631  0.00566644  0.02093405  0.00017574]\n",
      " [-0.01174674 -0.0067725  -0.00926798 -0.01903343]]\n",
      "<NDArray 2x4 @cpu(0)> \n",
      "output bias: \n",
      "[ 0.  0.]\n",
      "<NDArray 2 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "params = net.collect_params()\n",
    "params.initialize(init=init.Normal(sigma=0.02), force_reinit=True)\n",
    "print('hidden weight:', net.hidden.weight.data(), '\\nhidden bias:', net.hidden.bias.data(), '\\noutput weight:', \n",
    "      net.output.weight.data(), '\\noutput bias:', net.output.bias.data())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也可以把模型中任意层任意参数初始化，例如把上面模型中隐含层的偏差参数初始化为1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ 1.  1.  1.  1.]\n",
      "<NDArray 4 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "net.hidden.bias.initialize(init=init.One(), force_reinit=True)\n",
    "print(net.hidden.bias.data())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 自定义初始化方法\n",
    "通过重载_init_weight来实现自定义的初始化方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 14.80893517  11.18727684  16.88661194  13.1798315   18.80475998]\n",
       " [ 14.14262962  19.18235397  10.64147472  12.16822147  16.92472076]\n",
       " [ 15.65188885  15.66601467  18.65102577  12.65389442  15.08968925]\n",
       " [ 15.232481    19.1672287   10.93940544  19.21157646  15.75946522]]\n",
       "<NDArray 4x5 @cpu(0)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyInit(init.Initializer):\n",
    "    def __init__(self):\n",
    "        super(MyInit, self).__init__()\n",
    "        self._verbose = True\n",
    "    def _init_weight(self, _, arr):\n",
    "        #初始化权重，使用out=arr后我们不需指定形状\n",
    "        nd.random.uniform(low=10, high=20, out=arr)\n",
    "\n",
    "net = MLP()\n",
    "net.initialize(MyInit())\n",
    "net(x)\n",
    "net.hidden.weight.data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们还可以通过Parameter.set_data来直接写模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output layer default weight: \n",
      "[[-0.0296133   0.06470639 -0.00933967 -0.03517456]\n",
      " [ 0.03585494  0.01066203 -0.01454624  0.01288587]]\n",
      "<NDArray 2x4 @cpu(0)>\n",
      "output layer modified weight: \n",
      "[[ 1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.]]\n",
      "<NDArray 2x4 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "net = MLP()\n",
    "net.initialize()\n",
    "net(x)\n",
    "print('output layer default weight:', net.output.weight.data())\n",
    "\n",
    "w = net.output.weight\n",
    "w.set_data(nd.ones(w.shape))\n",
    "print('output layer modified weight:', net.output.weight.data())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "延后初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mlp6_ (\n",
       "  Parameter mlp6_dense0_weight (shape=(4, 0), dtype=<class 'numpy.float32'>)\n",
       "  Parameter mlp6_dense0_bias (shape=(4,), dtype=<class 'numpy.float32'>)\n",
       "  Parameter mlp6_dense1_weight (shape=(2, 0), dtype=<class 'numpy.float32'>)\n",
       "  Parameter mlp6_dense1_bias (shape=(2,), dtype=<class 'numpy.float32'>)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MLP()\n",
    "net.collect_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mlp6_ (\n",
       "  Parameter mlp6_dense0_weight (shape=(4, 0), dtype=<class 'numpy.float32'>)\n",
       "  Parameter mlp6_dense0_bias (shape=(4,), dtype=<class 'numpy.float32'>)\n",
       "  Parameter mlp6_dense1_weight (shape=(2, 0), dtype=<class 'numpy.float32'>)\n",
       "  Parameter mlp6_dense1_bias (shape=(2,), dtype=<class 'numpy.float32'>)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.initialize() #调用了initialize后模型参数未初始化\n",
    "net.collect_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[ 0.08712929  0.6481719   0.0202184   0.36824155  0.83261985]\n",
      " [ 0.95715517  0.77815676  0.14035077  0.87001216  0.87008727]\n",
      " [ 0.97861832  0.47360805  0.79915857  0.80091077  0.46147937]]\n",
      "<NDArray 3x5 @cpu(0)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "mlp6_ (\n",
       "\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(x)\n",
    "net(x) #输入数据x后，参数初始化了\n",
    "net.collect_params()\n",
    "net.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "共享模型参数\n",
    "\n",
    "当我们希望在模型的多个层之间共享模型参数时，可以通过nn.Block的params来指定模型参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[ 0.03488377 -0.00997238  0.05298331 -0.05103363]\n",
      " [-0.05559913 -0.02824048 -0.05706766  0.00979508]\n",
      " [-0.02043347  0.01272219  0.00725428  0.01040554]\n",
      " [-0.06529249  0.02144811  0.06565464  0.02129445]]\n",
      "<NDArray 4x4 @cpu(0)>\n",
      "\n",
      "[[ 0.03488377 -0.00997238  0.05298331 -0.05103363]\n",
      " [-0.05559913 -0.02824048 -0.05706766  0.00979508]\n",
      " [-0.02043347  0.01272219  0.00725428  0.01040554]\n",
      " [-0.06529249  0.02144811  0.06565464  0.02129445]]\n",
      "<NDArray 4x4 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "net = nn.Sequential()\n",
    "with net.name_scope():\n",
    "    net.add(nn.Dense(4, activation='relu'))\n",
    "    net.add(nn.Dense(4, activation='relu'))\n",
    "    #通过params指定需要共享的模型参数\n",
    "    net.add(nn.Dense(4, activation='relu', params=net[1].params))\n",
    "    net.add(nn.Dense(4))\n",
    "\n",
    "net.initialize()\n",
    "net(x)\n",
    "print(net[1].weight.data())\n",
    "print(net[2].weight.data())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们也可以在使用nn.Block构造的多层感知机中，让模型的第二隐含层hidden2和第三层hidden3共享模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[  1.35317594e-02  -6.55425489e-02   3.97101939e-02  -4.69428152e-02]\n",
      " [  3.68058681e-06   1.70069709e-02  -6.29481897e-02   1.08120069e-02]\n",
      " [  2.78737247e-02  -3.66950035e-02   6.89354911e-02   6.07899651e-02]\n",
      " [ -3.25832441e-02   1.59552321e-02   2.50726864e-02   4.98858839e-03]]\n",
      "<NDArray 4x4 @cpu(0)>\n",
      "\n",
      "[[  1.35317594e-02  -6.55425489e-02   3.97101939e-02  -4.69428152e-02]\n",
      " [  3.68058681e-06   1.70069709e-02  -6.29481897e-02   1.08120069e-02]\n",
      " [  2.78737247e-02  -3.66950035e-02   6.89354911e-02   6.07899651e-02]\n",
      " [ -3.25832441e-02   1.59552321e-02   2.50726864e-02   4.98858839e-03]]\n",
      "<NDArray 4x4 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "class MLP_SHARE(nn.Block):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MLP_SHARE, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.hidden1 = nn.Dense(4, activation='relu')\n",
    "            self.hidden2 = nn.Dense(4, activation='relu')\n",
    "            #通过params指定需要共享的模型参数\n",
    "            self.hidden3 = nn.Dense(4, activation='relu', params=self.hidden2.params)\n",
    "            self.output = nn.Dense(2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.output(self.hidden3(self.hidden2(self.hidden1(x))))\n",
    "net = MLP_SHARE()\n",
    "net.initialize()\n",
    "net(x)\n",
    "print(net.hidden2.weight.data())\n",
    "print(net.hidden3.weight.data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
